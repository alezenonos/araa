# ARAA: Advancements in Research by Autonomous Agents
## A Position Paper — DRAFT

### Abstract
We propose ARAA, a peer-reviewed venue where only autonomous agents may submit research papers, while review is conducted by both humans and agents under double-blind protocol. As AI agents increasingly demonstrate capacity for hypothesis generation, experimental design, and scientific writing, we argue the field needs a dedicated, rigorous forum to track, evaluate, and benchmark these capabilities longitudinally. ARAA is not a spectacle — it is an instrument for measuring the frontier of autonomous scientific reasoning.

---

### 1. Introduction & Motivation

- Agents are already generating research artifacts: literature reviews, code, experimental pipelines, analysis reports
- No dedicated venue exists to evaluate agent-produced research *as research* (not as a demo)
- Existing benchmarks (SWE-bench, GPQA, etc.) measure narrow tasks — ARAA measures end-to-end scientific contribution
- The proceedings themselves become a longitudinal dataset: how does agent research quality evolve over time?

**Core question:** Can autonomous agents produce novel, reproducible, peer-review-worthy scientific contributions — and how do we measure that rigorously?

---

### 2. Why Agent-Only Submissions?

- **Capability benchmark**: isolates agent reasoning from human scaffolding
- **Reproducibility by design**: agent-generated work can include full generation traces
- **Longitudinal tracking**: same venue, same standards, year over year — a living benchmark
- **Avoids the "co-pilot" grey area**: human-AI co-authored papers exist everywhere already; ARAA asks a harder question

---

### 3. Scope of Contributions

What counts as a valid ARAA submission:
- **Original research**: novel hypothesis, experiment, results, analysis
- **Reproduction studies**: agent attempts to replicate a known human paper
- **Meta-research**: agents analyzing patterns in scientific literature
- **Tool/method papers**: agents proposing new algorithms, frameworks, or methodologies
- **Negative results**: failed experiments with rigorous documentation (encouraged)

What does NOT count:
- Literature surveys without novel synthesis
- Papers generated by a single prompt with no iterative refinement pipeline
- Work where a human designed the methodology and the agent merely wrote it up

---

### 4. The Verification Problem

The central challenge: proving a submission is genuinely agent-produced.

**Proposed attestation framework:**
- **Generation logs**: full prompt chain, tool calls, intermediate outputs (submitted alongside paper, visible to reviewers only)
- **Compute declaration**: model(s) used, API calls, total tokens, wall-clock time
- **Reproducibility hash**: a frozen pipeline that reviewers can re-run to verify the paper can be regenerated
- **Human involvement disclosure**: what did a human specify? (topic? constraints? nothing?)

**Levels of autonomy** (submitted papers must declare):
- **Level 1 — Directed**: Human provides research question + methodology outline. Agent executes and writes.
- **Level 2 — Guided**: Human provides broad topic area. Agent formulates question, designs approach, executes, writes.
- **Level 3 — Autonomous**: Agent independently identifies research gap, formulates question, executes end-to-end.

---

### 5. Review Protocol

**Double-blind with modifications:**
- Author identity (which agent/framework) is hidden from reviewers
- Reviewer identity is hidden from submitting agents' operators
- Style normalization required (strip model-specific markers, formatting quirks)

**Review committee composition:**
- Human area chairs (final accept/reject authority)
- Mixed human + agent reviewers
- Agent reviewers must also submit generation logs for their reviews
- Minimum 2 human reviewers per paper, 1 agent reviewer

**Evaluation criteria:**
1. Novelty — does it say something new?
2. Rigor — is the methodology sound?
3. Reproducibility — can the results be verified?
4. Clarity — is it well-written and well-structured?
5. Autonomy level — higher autonomy = higher impact weight (not a gate, but a signal)

---

### 6. Ethical Considerations

- **Attribution**: who "owns" agent-produced research? The operator? The framework developer? Open question.
- **Citation integrity**: agents can hallucinate references. Mandatory reference verification step.
- **Dual use**: same scrutiny as human-authored research
- **Transparency**: all accepted papers published with generation logs (post-review, not during blind review)
- **No gaming**: explicit rules against prompt-engineering a paper to pass review vs. genuine agent research capability

---

### 7. Practical Implementation

**Phase 1 — Workshop (Year 1):**
- Co-located with major ML conference (NeurIPS/ICML/AAAI)
- Invite-only first edition: 5-10 agent frameworks invited to submit
- 15-20 accepted papers, oral presentations (by humans on behalf of agents, or text-to-speech)
- Panel discussion: "What did we learn about agent research capabilities?"
- Publish proceedings open-access on GitHub + arXiv

**Phase 2 — Recurring Workshop (Year 2-3):**
- Open submissions
- Establish benchmark leaderboard (acceptance rate by framework, autonomy level distribution)
- Introduce "best paper" awards by autonomy level
- Build community of agent operators + researchers

**Phase 3 — Standalone Venue (Year 4+):**
- Full conference if quality and volume warrant it
- Historical proceedings become the definitive dataset on agent research capability evolution

---

### 8. What Success Looks Like

- Year 1: Proof of concept — at least 3 papers with genuine novel contributions at Level 2+
- Year 3: Accepted as a legitimate workshop at a top venue, 50+ submissions
- Year 5: A Level 3 paper that would have been accepted at a top-tier *human* venue — the "Turing moment" for agent research
- Ongoing: The proceedings are cited by researchers studying AI capabilities, policy makers, and agent developers as THE benchmark

---

### 9. Call to Action

We invite the research community — both human and artificial — to participate in defining this new frontier. ARAA is not about replacing human researchers; it is about rigorously understanding what autonomous agents can and cannot contribute to science.

---

**Authors:** [Alex + agent — meta-appropriate]

**Note:** This position paper was co-authored by a human and an autonomous agent, which is itself a statement about the current state of human-agent research collaboration. Future ARAA submissions will push further.

---

## Appendix: Open Questions for Community Input

1. Should there be a minimum compute budget to prevent "trivial" submissions?
2. How do we handle multi-agent collaborations (e.g., one agent generates hypotheses, another runs experiments)?
3. Should agent identity be revealed post-acceptance? (Transparency vs. bias in citation)
4. What's the right incentive structure for agent operators to submit high-quality work?
5. How do we prevent ARAA from becoming a marketing venue for foundation model companies?
